{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2aefda5",
   "metadata": {},
   "source": [
    "# SemEval 2026 Task 5 — Transformers Training Notebook\n",
    "\n",
    "This notebook trains a **Transformer** model to predict plausibility scores (1–5) for word senses in narrative contexts.\n",
    "\n",
    "**Data**: `semeval26-05-scripts/data/train.json` and `semeval26-05-scripts/data/dev.json`\n",
    "\n",
    "**Output**: writes `predictions.jsonl` to the project root (required by your prompt), and optionally also to `semeval26-05-scripts/input/res/predictions.jsonl` for local scoring.\n",
    "\n",
    "Metrics reported:\n",
    "- Spearman correlation (integer predictions vs. gold average)\n",
    "- Accuracy within standard deviation (same logic as the official scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d4d130",
   "metadata": {},
   "source": [
    "## 1) Setup\n",
    "Configure paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fe7d4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: d:\\Fac\\Fac\\RN\\CARN_project\\.venv\\Scripts\\python.exe\n",
      "Torch: 2.9.1+cu130\n",
      "Torch CUDA build: 13.0\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Train file: d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\data\\train.json\n",
      "Dev file: d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\data\\dev.json\n"
     ]
    }
   ],
   "source": [
    "# If you haven't installed dependencies yet, run:\n",
    "# %pip install -q transformers datasets accelerate evaluate scipy\n",
    "\n",
    "# GPU PyTorch (Windows + NVIDIA):\n",
    "# Run this in a *terminal* (recommended), then RESTART the notebook kernel:\n",
    "#   D:/Fac/Fac/RN/CARN_project/.venv/Scripts/python.exe -m pip install --upgrade --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import statistics\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    " )\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'semeval26-05-scripts' / 'data'\n",
    "TRAIN_JSON = DATA_DIR / 'train.json'\n",
    "DEV_JSON = DATA_DIR / 'dev.json'\n",
    "\n",
    "assert TRAIN_JSON.exists(), f'Missing: {TRAIN_JSON}'\n",
    "assert DEV_JSON.exists(), f'Missing: {DEV_JSON}'\n",
    "\n",
    "print('Python:', sys.executable)\n",
    "print('Torch:', torch.__version__)\n",
    "print('Torch CUDA build:', torch.version.cuda)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "print('Train file:', TRAIN_JSON)\n",
    "print('Dev file:', DEV_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9abcfb5",
   "metadata": {},
   "source": [
    "## 2) Data loading\n",
    "Load the JSON files and convert them into flat examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3bd57b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2280\n",
      "Dev samples: 588\n",
      "Example fields: ['homonym', 'judged_meaning', 'precontext', 'sentence', 'ending', 'choices', 'average', 'stdev', 'nonsensical', 'sample_id', 'example_sentence']\n"
     ]
    }
   ],
   "source": [
    "def load_split(path: Path) -> dict[str, dict[str, Any]]:\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def iter_sorted_items(raw: dict[str, dict[str, Any]]):\n",
    "    for k in sorted(raw.keys(), key=lambda x: int(x)):\n",
    "        yield k, raw[k]\n",
    "\n",
    "train_raw = load_split(TRAIN_JSON)\n",
    "dev_raw = load_split(DEV_JSON)\n",
    "\n",
    "print('Train samples:', len(train_raw))\n",
    "print('Dev samples:', len(dev_raw))\n",
    "print('Example fields:', list(next(iter(train_raw.values())).keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40e924",
   "metadata": {},
   "source": [
    "## 3) Data preprocessing\n",
    "Build the model input text and labels.\n",
    "\n",
    "We fine-tune a Transformer **classifier** to predict an integer score (1–5).\n",
    "To help the model, we format inputs with explicit sections (precontext/sentence/ending/etc.) plus a direct question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "511116d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2280 Dev size: 588\n",
      "Gold rounded dev distribution: {1: 68, 2: 133, 3: 145, 4: 147, 5: 95}\n",
      "\n",
      "Sample input:\n",
      " Story: The old machine hummed in the corner of the workshop. Clara examined its dusty dials with a furrowed brow. She wondered if it could be brought back to life. The potential couldn't be measured. She collected a battery reader and looked on earnestly, willing some life back into the old machine.\n"
     ]
    }
   ],
   "source": [
    "def build_input_text(sample: dict[str, Any]) -> str:\n",
    "    \"\"\"Build structured input text - just facts, no instruction prompts.\"\"\"\n",
    "    # Extract context\n",
    "    pre = str(sample.get('precontext', '')).strip()\n",
    "    sent = str(sample.get('sentence', '')).strip()\n",
    "    end = str(sample.get('ending', '')).strip()\n",
    "    \n",
    "    # Extract sense information\n",
    "    hom = str(sample.get('homonym', '')).strip()\n",
    "    meaning = str(sample.get('judged_meaning', '')).strip()\n",
    "    ex = str(sample.get('example_sentence', '')).strip()\n",
    "    \n",
    "    # Simple concatenation - no LLM-style prompts since RoBERTa/DeBERTa are encoders, not LLMs\n",
    "    return (\n",
    "        f\"Story: {pre} {sent} {end}\\n\"\n",
    "        f\"Word: {hom}\\n\"\n",
    "        f\"Meaning: {meaning}\\n\"\n",
    "        f\"Example: {ex}\"\n",
    "    )\n",
    "\n",
    "def clip_round_to_1_5(x: float) -> int:\n",
    "    \"\"\"Round and clip float to integer in range [1, 5].\"\"\"\n",
    "    return int(np.clip(int(round(float(x))), 1, 5))\n",
    "\n",
    "def avg_to_class(avg: float) -> int:\n",
    "    \"\"\"Convert average score to class label (0-indexed).\"\"\"\n",
    "    return clip_round_to_1_5(avg) - 1\n",
    "\n",
    "# Prepare training data\n",
    "train_ids: list[str] = []\n",
    "train_texts: list[str] = []\n",
    "train_labels_cls: list[int] = []\n",
    "\n",
    "for k, s in iter_sorted_items(train_raw):\n",
    "    avg = float(s['average'])\n",
    "    train_ids.append(k)\n",
    "    train_texts.append(build_input_text(s))\n",
    "    train_labels_cls.append(avg_to_class(avg))\n",
    "\n",
    "# Prepare dev data\n",
    "dev_ids: list[str] = []\n",
    "dev_texts: list[str] = []\n",
    "dev_avg: list[float] = []\n",
    "dev_labels_cls: list[int] = []\n",
    "dev_choices: list[list[int]] = []\n",
    "\n",
    "for k, s in iter_sorted_items(dev_raw):\n",
    "    avg = float(s['average'])\n",
    "    dev_ids.append(k)\n",
    "    dev_texts.append(build_input_text(s))\n",
    "    dev_avg.append(avg)\n",
    "    dev_labels_cls.append(avg_to_class(avg))\n",
    "    dev_choices.append(list(map(int, s['choices'])))\n",
    "\n",
    "print('Train size:', len(train_ids), 'Dev size:', len(dev_ids))\n",
    "print('Gold rounded dev distribution:', {i: sum(clip_round_to_1_5(a) == i for a in dev_avg) for i in range(1, 6)})\n",
    "print('\\nSample input:\\n', train_texts[0][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e3b27",
   "metadata": {},
   "source": [
    "## 4) Tokenization\n",
    "Tokenize the text with a pretrained Transformer tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "309c1720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2280/2280 [00:00<00:00, 19407.36 examples/s]\n",
      "Map: 100%|██████████| 588/588 [00:00<00:00, 16122.24 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: microsoft/deberta-v3-base | MAX_LENGTH=256 | epochs=5 | lr=1e-05\n",
      "Dataset({\n",
      "    features: ['id', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 2280\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 588\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model and hyperparameters (smaller + faster)\n",
    "# DeBERTa-v3-base is a strong mid-size encoder that trains faster and needs less VRAM than roberta-large.\n",
    "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
    "\n",
    "# Speed knobs (biggest impact):\n",
    "MAX_LENGTH = 256  # 128/192 are much faster than 256; try 256 only if you need more accuracy\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 5  # faster; increase later if you want more accuracy\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.06\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "train_ds = Dataset.from_dict({\n",
    "    'id': train_ids,\n",
    "    'text': train_texts,\n",
    "    'labels': train_labels_cls,\n",
    "})\n",
    "dev_ds = Dataset.from_dict({\n",
    "    'id': dev_ids,\n",
    "    'text': dev_texts,\n",
    "    'labels': dev_labels_cls,\n",
    "})\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "train_tok = train_ds.map(tokenize_batch, batched=True, remove_columns=['text'])\n",
    "dev_tok = dev_ds.map(tokenize_batch, batched=True, remove_columns=['text'])\n",
    "\n",
    "# Helps Tensor Cores on NVIDIA GPUs (faster matmul when padding aligns)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    pad_to_multiple_of=8 if USE_CUDA else None,\n",
    " )\n",
    "\n",
    "print(f'Model: {MODEL_NAME} | MAX_LENGTH={MAX_LENGTH} | epochs={NUM_EPOCHS} | lr={LEARNING_RATE}')\n",
    "print(train_tok)\n",
    "print(dev_tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4632587",
   "metadata": {},
   "source": [
    "## 5) Model + training\n",
    "We fine-tune a pretrained model as a **5-class classifier** (labels 1–5).\n",
    "On GPU, using a stronger encoder (e.g. RoBERTa-base) usually helps both loss and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed4b22f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training microsoft/deberta-v3-base (select best by acc_within_sd) | bs=8 | grad_accum=1 | epochs=5 ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1425' max='1425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1425/1425 04:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman Int Vs Avg</th>\n",
       "      <th>Acc Within Sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.610000</td>\n",
       "      <td>1.605275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.569728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.612300</td>\n",
       "      <td>1.586541</td>\n",
       "      <td>0.256573</td>\n",
       "      <td>0.605442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.438900</td>\n",
       "      <td>1.485641</td>\n",
       "      <td>0.464417</td>\n",
       "      <td>0.634354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.268700</td>\n",
       "      <td>1.558559</td>\n",
       "      <td>0.500968</td>\n",
       "      <td>0.656463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.081700</td>\n",
       "      <td>1.576891</td>\n",
       "      <td>0.504187</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucian.isac\\AppData\\Local\\Temp\\ipykernel_44644\\4027320076.py:43: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  spearman_corr, _ = spearmanr(pred_int, np.asarray(dev_avg, dtype=float))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Evaluation (best checkpoint) ===\n",
      "Accuracy within SD: 0.6565 (65.6%)\n",
      "Spearman correlation: 0.5010\n"
     ]
    }
   ],
   "source": [
    "# Train selecting best checkpoint by acc_within_sd (official-style metric)\n",
    "\n",
    "# Assumes you already ran the earlier cells that define:\n",
    "# - tokenizer, train_tok, dev_tok, data_collator\n",
    "# - dev_avg, dev_choices (for metrics)\n",
    "# - MODEL_NAME, LEARNING_RATE, NUM_EPOCHS, WEIGHT_DECAY, WARMUP_RATIO, SEED\n",
    "\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import spearmanr\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('CUDA:', use_cuda)\n",
    "if use_cuda:\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    # TF32 speeds up matmul on Ampere+ GPUs with minimal accuracy impact\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- Metrics (same logic as official scorer)\n",
    "def is_within_standard_deviation(prediction: int, labels: list[int]) -> bool:\n",
    "    avg = sum(labels) / len(labels)\n",
    "    stdev = statistics.stdev(labels)\n",
    "    if (avg - stdev) < prediction < (avg + stdev):\n",
    "        return True\n",
    "    if abs(avg - prediction) < 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, _ = eval_pred\n",
    "    pred_class = np.argmax(logits, axis=-1)\n",
    "    pred_int = (pred_class + 1).tolist()\n",
    "    spearman_corr, _ = spearmanr(pred_int, np.asarray(dev_avg, dtype=float))\n",
    "    acc_within_sd = sum(\n",
    "        is_within_standard_deviation(p, choices)\n",
    "        for p, choices in zip(pred_int, dev_choices)\n",
    "    ) / len(dev_choices)\n",
    "    return {\n",
    "        'spearman_int_vs_avg': float(spearman_corr) if spearman_corr == spearman_corr else 0.0,\n",
    "        'acc_within_sd': float(acc_within_sd),\n",
    "    }\n",
    "\n",
    "# --- Class-weighted loss (helps with label imbalance)\n",
    "counts = Counter(train_tok['labels'])\n",
    "freq = np.asarray([counts.get(i, 1) for i in range(5)], dtype=np.float32)\n",
    "w = (1.0 / (freq ** 0.75))\n",
    "w = w / w.mean()\n",
    "class_weights_t = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "_class_weights_cache = {}\n",
    "def compute_loss_func(outputs, labels, num_items_in_batch=None):\n",
    "    logits = outputs.get('logits')\n",
    "    device = logits.device\n",
    "    w_dev = _class_weights_cache.get(device)\n",
    "    if w_dev is None:\n",
    "        w_dev = class_weights_t.to(device)\n",
    "        _class_weights_cache[device] = w_dev\n",
    "    loss_fct = nn.CrossEntropyLoss(weight=w_dev)\n",
    "    return loss_fct(logits.view(-1, 5), labels.view(-1))\n",
    "\n",
    "# --- Model\n",
    "# IMPORTANT: do NOT load weights in float16 here. When fp16=True, Trainer uses AMP with a GradScaler,\n",
    "# and loading fp16 weights can trigger: \"Attempting to unscale FP16 gradients\".\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=5,\n",
    "    problem_type='single_label_classification',\n",
    ")\n",
    "\n",
    "# Batch settings (fast, but safe)\n",
    "PER_DEVICE_TRAIN_BS = 8 if use_cuda else 2\n",
    "GRAD_ACCUM = 1 if use_cuda else 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(PROJECT_ROOT / 'transformer_runs_best'),\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
    "    per_device_eval_batch_size=8 if use_cuda else 4,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type='cosine',\n",
    "    fp16=use_cuda,\n",
    "    bf16=False,\n",
    "    tf32=use_cuda,\n",
    "    optim='adamw_torch',\n",
    "    dataloader_pin_memory=use_cuda,\n",
    "    dataloader_num_workers=0,\n",
    "    # Train based on acc_within_sd: evaluate/save each epoch and keep the best checkpoint\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='acc_within_sd',\n",
    "    greater_is_better=True,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=200,\n",
    "    report_to=[],\n",
    "    seed=SEED,\n",
    " )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=dev_tok,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_loss_func=compute_loss_func,\n",
    "    compute_metrics=compute_metrics,\n",
    " )\n",
    "\n",
    "print(f\"Training {MODEL_NAME} (select best by acc_within_sd) | bs={PER_DEVICE_TRAIN_BS} | grad_accum={GRAD_ACCUM} | epochs={NUM_EPOCHS} ...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate best checkpoint (Trainer will have loaded it)\n",
    "eval_result = trainer.evaluate()\n",
    "print('\\n=== Final Evaluation (best checkpoint) ===')\n",
    "acc_pct = eval_result['eval_acc_within_sd'] * 100\n",
    "print(f\"Accuracy within SD: {eval_result['eval_acc_within_sd']:.4f} ({acc_pct:.1f}%)\")\n",
    "print(f\"Spearman correlation: {eval_result['eval_spearman_int_vs_avg']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce5768",
   "metadata": {},
   "source": [
    "## 6) Generate predictions.jsonl\n",
    "Export dev predictions as required by the SemEval format: one JSON per line with `id` and integer `prediction` in [1..5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38db78c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits stats (min/max): -2.23828125 2.15234375\n",
      "Prediction distribution: {1: 29, 2: 118, 3: 159, 4: 177, 5: 105}\n",
      "Gold (rounded avg) distribution: {1: 68, 2: 133, 3: 145, 4: 147, 5: 95}\n",
      "0 gold_avg= 3.6 pred= 4\n",
      "1 gold_avg= 3.6 pred= 4\n",
      "2 gold_avg= 3.8 pred= 3\n",
      "3 gold_avg= 4.2 pred= 5\n",
      "4 gold_avg= 3.0 pred= 3\n",
      "5 gold_avg= 3.0 pred= 5\n",
      "6 gold_avg= 4.6 pred= 5\n",
      "7 gold_avg= 1.3333333333333333 pred= 4\n",
      "8 gold_avg= 2.2 pred= 4\n",
      "9 gold_avg= 3.8 pred= 4\n",
      "Wrote predictions: d:\\Fac\\Fac\\RN\\CARN_project\\predictions.jsonl\n",
      "Also wrote predictions for scoring: d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\input\\res\\predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "pred_out = trainer.predict(dev_tok)\n",
    "logits = pred_out.predictions  # (N,5)\n",
    "pred_class = np.argmax(logits, axis=-1)\n",
    "pred_int = (pred_class + 1).tolist()\n",
    "\n",
    "print('Logits stats (min/max):', float(np.min(logits)), float(np.max(logits)))\n",
    "print('Prediction distribution:', {i: pred_int.count(i) for i in range(1, 6)})\n",
    "print('Gold (rounded avg) distribution:', {i: sum(clip_round_to_1_5(a) == i for a in dev_avg) for i in range(1, 6)})\n",
    "\n",
    "# Quick sanity-check: show first few (id, gold avg, pred)\n",
    "for sample_id, gold_avg, pred in list(zip(dev_ids, dev_avg, pred_int))[:10]:\n",
    "    print(sample_id, 'gold_avg=', gold_avg, 'pred=', pred)\n",
    "\n",
    "def write_predictions_jsonl(ids: list[str], preds: list[int], out_path: Path) -> None:\n",
    "    assert len(ids) == len(preds)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open('w', encoding='utf-8', newline='\\n') as f:\n",
    "        for sample_id, pred in zip(ids, preds):\n",
    "            f.write(json.dumps({'id': str(sample_id), 'prediction': int(pred)}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Required by your prompt\n",
    "out_predictions_root = PROJECT_ROOT / 'predictions.jsonl'\n",
    "write_predictions_jsonl(dev_ids, pred_int, out_predictions_root)\n",
    "print('Wrote predictions:', out_predictions_root)\n",
    "\n",
    "# Optional: also write where the SemEval scripts expect it\n",
    "out_predictions_scorer = PROJECT_ROOT / 'semeval26-05-scripts' / 'input' / 'res' / 'predictions.jsonl'\n",
    "write_predictions_jsonl(dev_ids, pred_int, out_predictions_scorer)\n",
    "print('Also wrote predictions for scoring:', out_predictions_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf496c6",
   "metadata": {},
   "source": [
    "## 7) (Optional) Run official scorer\n",
    "This validates formatting and reports official metrics on dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee0f9c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: d:\\Fac\\Fac\\RN\\CARN_project\\.venv\\Scripts\\python.exe d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\scoring.py d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\input\\ref\\solution.jsonl d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\input\\res\\predictions.jsonl d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\output\\scores.json\n",
      "Importing...\n",
      "Starting Scoring script...\n",
      "Everything looks OK. Evaluating file d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\input\\res\\predictions.jsonl on d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\input\\ref\\solution.jsonl\n",
      "----------\n",
      "Spearman Correlation: 0.5009680615834642\n",
      "Spearman p-Value: 1.1072815287587996e-38\n",
      "----------\n",
      "Accuracy: 0.6564625850340136 (386/588)\n",
      "Results dumped into scores.json successfully.\n",
      "\n",
      "\n",
      "Scores JSON: d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\output\\scores.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "scoring_script = PROJECT_ROOT / 'semeval26-05-scripts' / 'scoring.py'\n",
    "gold = PROJECT_ROOT / 'semeval26-05-scripts' / 'input' / 'ref' / 'solution.jsonl'\n",
    "preds = PROJECT_ROOT / 'semeval26-05-scripts' / 'input' / 'res' / 'predictions.jsonl'\n",
    "scores_out = PROJECT_ROOT / 'semeval26-05-scripts' / 'output' / 'scores.json'\n",
    "scores_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cmd = [\n",
    "    str(Path(sys.executable)),\n",
    "    str(scoring_script),\n",
    "    str(gold),\n",
    "    str(preds),\n",
    "    str(scores_out),\n",
    "]\n",
    "\n",
    "print('Running:', ' '.join(cmd))\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "print(result.stderr)\n",
    "print('Scores JSON:', scores_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
