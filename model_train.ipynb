{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2aefda5",
   "metadata": {},
   "source": [
    "# SemEval 2026 Task 5 — Transformers Training Notebook\n",
    "\n",
    "This notebook trains a **Transformer** model to predict plausibility scores (1–5) for word senses in narrative contexts.\n",
    "\n",
    "**Data**: `semeval26-05-scripts/data/train.json` and `semeval26-05-scripts/data/dev.json`\n",
    "\n",
    "**Output**: writes `predictions.jsonl` to the project root (required by your prompt), and optionally also to `semeval26-05-scripts/input/res/predictions.jsonl` for local scoring.\n",
    "\n",
    "Metrics reported:\n",
    "- Spearman correlation (integer predictions vs. gold average)\n",
    "- Accuracy within standard deviation (same logic as the official scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d4d130",
   "metadata": {},
   "source": [
    "## 1) Setup\n",
    "Configure paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fe7d4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: d:\\Fac\\Fac\\RN\\CARN_project\\.venv\\Scripts\\python.exe\n",
      "Torch: 2.9.1+cu130\n",
      "Torch CUDA build: 13.0\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Train file: d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\data\\train.json\n",
      "Dev file: d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\data\\dev.json\n"
     ]
    }
   ],
   "source": [
    "# If you haven't installed dependencies yet, run:\n",
    "# %pip install -q transformers datasets accelerate evaluate scipy\n",
    "\n",
    "# GPU PyTorch (Windows + NVIDIA):\n",
    "# Run this in a *terminal* (recommended), then RESTART the notebook kernel:\n",
    "#   D:/Fac/Fac/RN/CARN_project/.venv/Scripts/python.exe -m pip install --upgrade --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import statistics\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    " )\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'semeval26-05-scripts' / 'data'\n",
    "TRAIN_JSON = DATA_DIR / 'train.json'\n",
    "DEV_JSON = DATA_DIR / 'dev.json'\n",
    "\n",
    "assert TRAIN_JSON.exists(), f'Missing: {TRAIN_JSON}'\n",
    "assert DEV_JSON.exists(), f'Missing: {DEV_JSON}'\n",
    "\n",
    "print('Python:', sys.executable)\n",
    "print('Torch:', torch.__version__)\n",
    "print('Torch CUDA build:', torch.version.cuda)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "print('Train file:', TRAIN_JSON)\n",
    "print('Dev file:', DEV_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9abcfb5",
   "metadata": {},
   "source": [
    "## 2) Data loading\n",
    "Load the JSON files and convert them into flat examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3bd57b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2280\n",
      "Dev samples: 588\n",
      "Example fields: ['homonym', 'judged_meaning', 'precontext', 'sentence', 'ending', 'choices', 'average', 'stdev', 'nonsensical', 'sample_id', 'example_sentence']\n"
     ]
    }
   ],
   "source": [
    "def load_split(path: Path) -> dict[str, dict[str, Any]]:\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def iter_sorted_items(raw: dict[str, dict[str, Any]]):\n",
    "    for k in sorted(raw.keys(), key=lambda x: int(x)):\n",
    "        yield k, raw[k]\n",
    "\n",
    "train_raw = load_split(TRAIN_JSON)\n",
    "dev_raw = load_split(DEV_JSON)\n",
    "\n",
    "print('Train samples:', len(train_raw))\n",
    "print('Dev samples:', len(dev_raw))\n",
    "print('Example fields:', list(next(iter(train_raw.values())).keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40e924",
   "metadata": {},
   "source": [
    "## 3) Data preprocessing\n",
    "Build the model input text and labels.\n",
    "\n",
    "We fine-tune a Transformer **classifier** to predict an integer score (1–5).\n",
    "To help the model, we format inputs with explicit sections (precontext/sentence/ending/etc.) plus a direct question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "511116d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2280 Dev size: 588\n",
      "Gold rounded dev distribution: {1: 68, 2: 133, 3: 145, 4: 147, 5: 95}\n",
      "\n",
      "Sample input:\n",
      " Story: The old machine hummed in the corner of the workshop. Clara examined its dusty dials with a furrowed brow. She wondered if it could be brought back to life. The potential couldn't be measured. She collected a battery reader and looked on earnestly, willing some life back into the old machine.\n"
     ]
    }
   ],
   "source": [
    "def build_input_text(sample: dict[str, Any]) -> str:\n",
    "    \"\"\"Build structured input text - just facts, no instruction prompts.\"\"\"\n",
    "    # Extract context\n",
    "    pre = str(sample.get('precontext', '')).strip()\n",
    "    sent = str(sample.get('sentence', '')).strip()\n",
    "    end = str(sample.get('ending', '')).strip()\n",
    "    \n",
    "    # Extract sense information\n",
    "    hom = str(sample.get('homonym', '')).strip()\n",
    "    meaning = str(sample.get('judged_meaning', '')).strip()\n",
    "    ex = str(sample.get('example_sentence', '')).strip()\n",
    "    \n",
    "    # Simple concatenation - no LLM-style prompts since RoBERTa/DeBERTa are encoders, not LLMs\n",
    "    return (\n",
    "        f\"Story: {pre} {sent} {end}\\n\"\n",
    "        f\"Word: {hom}\\n\"\n",
    "        f\"Meaning: {meaning}\\n\"\n",
    "        f\"Example: {ex}\"\n",
    "    )\n",
    "\n",
    "def clip_round_to_1_5(x: float) -> int:\n",
    "    \"\"\"Round and clip float to integer in range [1, 5].\"\"\"\n",
    "    return int(np.clip(int(round(float(x))), 1, 5))\n",
    "\n",
    "def avg_to_class(avg: float) -> int:\n",
    "    \"\"\"Convert average score to class label (0-indexed).\"\"\"\n",
    "    return clip_round_to_1_5(avg) - 1\n",
    "\n",
    "# Prepare training data\n",
    "train_ids: list[str] = []\n",
    "train_texts: list[str] = []\n",
    "train_labels_cls: list[int] = []\n",
    "\n",
    "for k, s in iter_sorted_items(train_raw):\n",
    "    avg = float(s['average'])\n",
    "    train_ids.append(k)\n",
    "    train_texts.append(build_input_text(s))\n",
    "    train_labels_cls.append(avg_to_class(avg))\n",
    "\n",
    "# Prepare dev data\n",
    "dev_ids: list[str] = []\n",
    "dev_texts: list[str] = []\n",
    "dev_avg: list[float] = []\n",
    "dev_labels_cls: list[int] = []\n",
    "dev_choices: list[list[int]] = []\n",
    "\n",
    "for k, s in iter_sorted_items(dev_raw):\n",
    "    avg = float(s['average'])\n",
    "    dev_ids.append(k)\n",
    "    dev_texts.append(build_input_text(s))\n",
    "    dev_avg.append(avg)\n",
    "    dev_labels_cls.append(avg_to_class(avg))\n",
    "    dev_choices.append(list(map(int, s['choices'])))\n",
    "\n",
    "print('Train size:', len(train_ids), 'Dev size:', len(dev_ids))\n",
    "print('Gold rounded dev distribution:', {i: sum(clip_round_to_1_5(a) == i for a in dev_avg) for i in range(1, 6)})\n",
    "print('\\nSample input:\\n', train_texts[0][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e3b27",
   "metadata": {},
   "source": [
    "## 4) Tokenization\n",
    "Tokenize the text with a pretrained Transformer tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "309c1720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2280/2280 [00:00<00:00, 19407.36 examples/s]\n",
      "Map: 100%|██████████| 588/588 [00:00<00:00, 16122.24 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: microsoft/deberta-v3-base | MAX_LENGTH=256 | epochs=5 | lr=1e-05\n",
      "Dataset({\n",
      "    features: ['id', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 2280\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 588\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model and hyperparameters (smaller + faster)\n",
    "# DeBERTa-v3-base is a strong mid-size encoder that trains faster and needs less VRAM than roberta-large.\n",
    "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
    "\n",
    "# Speed knobs (biggest impact):\n",
    "MAX_LENGTH = 256  # 128/192 are much faster than 256; try 256 only if you need more accuracy\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 5  # faster; increase later if you want more accuracy\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.06\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "train_ds = Dataset.from_dict({\n",
    "    'id': train_ids,\n",
    "    'text': train_texts,\n",
    "    'labels': train_labels_cls,\n",
    "})\n",
    "dev_ds = Dataset.from_dict({\n",
    "    'id': dev_ids,\n",
    "    'text': dev_texts,\n",
    "    'labels': dev_labels_cls,\n",
    "})\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "train_tok = train_ds.map(tokenize_batch, batched=True, remove_columns=['text'])\n",
    "dev_tok = dev_ds.map(tokenize_batch, batched=True, remove_columns=['text'])\n",
    "\n",
    "# Helps Tensor Cores on NVIDIA GPUs (faster matmul when padding aligns)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    pad_to_multiple_of=8 if USE_CUDA else None,\n",
    " )\n",
    "\n",
    "print(f'Model: {MODEL_NAME} | MAX_LENGTH={MAX_LENGTH} | epochs={NUM_EPOCHS} | lr={LEARNING_RATE}')\n",
    "print(train_tok)\n",
    "print(dev_tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4632587",
   "metadata": {},
   "source": [
    "## 5) Model + training\n",
    "We fine-tune a pretrained model as a **5-class classifier** (labels 1–5).\n",
    "On GPU, using a stronger encoder (e.g. RoBERTa-base) usually helps both loss and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b22f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training microsoft/deberta-v3-base (select best by acc_within_sd) | bs=8 | grad_accum=1 | epochs=5 ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='286' max='1425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 285/1425 00:48 < 03:14, 5.87 it/s, Epoch 1.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman Int Vs Avg</th>\n",
       "      <th>Acc Within Sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.608300</td>\n",
       "      <td>1.607482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.569728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucian.isac\\AppData\\Local\\Temp\\ipykernel_44644\\4027320076.py:43: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  spearman_corr, _ = spearmanr(pred_int, np.asarray(dev_avg, dtype=float))\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: I/O error: There is not enough space on the disk. (os error 112)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    114\u001b[39m trainer = Trainer(\n\u001b[32m    115\u001b[39m     model=model,\n\u001b[32m    116\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m    123\u001b[39m  )\n\u001b[32m    125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (select best by acc_within_sd) | bs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPER_DEVICE_TRAIN_BS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | grad_accum=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGRAD_ACCUM\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | epochs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Evaluate best checkpoint (Trainer will have loaded it)\u001b[39;00m\n\u001b[32m    129\u001b[39m eval_result = trainer.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Fac\\Fac\\RN\\CARN_project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Fac\\Fac\\RN\\CARN_project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2790\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2787\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2789\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2790\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2792\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2795\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2796\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Fac\\Fac\\RN\\CARN_project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3228\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3225\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3228\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3229\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Fac\\Fac\\RN\\CARN_project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3325\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3323\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   3324\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m3325\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy \u001b[38;5;129;01min\u001b[39;00m [SaveStrategy.STEPS, SaveStrategy.EPOCH] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_global_step:\n\u001b[32m   3328\u001b[39m     \u001b[38;5;66;03m# Wait for everyone to get here so we are sure the model has been saved by process 0\u001b[39;00m\n\u001b[32m   3329\u001b[39m     \u001b[38;5;66;03m# before we check if the best_checkpoint_dir exists\u001b[39;00m\n\u001b[32m   3330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Fac\\Fac\\RN\\CARN_project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4227\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   4224\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   4226\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m4227\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4229\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   4230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Fac\\Fac\\RN\\CARN_project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4331\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   4329\u001b[39m             torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n\u001b[32m   4330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4331\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4332\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_safetensors\u001b[49m\n\u001b[32m   4333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4336\u001b[39m     \u001b[38;5;28mself\u001b[39m.processing_class.save_pretrained(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Fac\\Fac\\RN\\CARN_project\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4173\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   4168\u001b[39m     gc.collect()\n\u001b[32m   4170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   4171\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   4172\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4173\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4175\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Fac\\Fac\\RN\\CARN_project\\.venv\\Lib\\site-packages\\safetensors\\torch.py:307\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    277\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    278\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    279\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    280\u001b[39m ):\n\u001b[32m    281\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    283\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    305\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: I/O error: There is not enough space on the disk. (os error 112)"
     ]
    }
   ],
   "source": [
    "# Train selecting best checkpoint by acc_within_sd (official-style metric)\n",
    "\n",
    "# Assumes you already ran the earlier cells that define:\n",
    "# - tokenizer, train_tok, dev_tok, data_collator\n",
    "# - dev_avg, dev_choices (for metrics)\n",
    "# - MODEL_NAME, LEARNING_RATE, NUM_EPOCHS, WEIGHT_DECAY, WARMUP_RATIO, SEED\n",
    "\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import spearmanr\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('CUDA:', use_cuda)\n",
    "if use_cuda:\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    # TF32 speeds up matmul on Ampere+ GPUs with minimal accuracy impact\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- Metrics (same logic as official scorer)\n",
    "def is_within_standard_deviation(prediction: int, labels: list[int]) -> bool:\n",
    "    avg = sum(labels) / len(labels)\n",
    "    stdev = statistics.stdev(labels)\n",
    "    if (avg - stdev) < prediction < (avg + stdev):\n",
    "        return True\n",
    "    if abs(avg - prediction) < 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, _ = eval_pred\n",
    "    pred_class = np.argmax(logits, axis=-1)\n",
    "    pred_int = (pred_class + 1).tolist()\n",
    "    spearman_corr, _ = spearmanr(pred_int, np.asarray(dev_avg, dtype=float))\n",
    "    acc_within_sd = sum(\n",
    "        is_within_standard_deviation(p, choices)\n",
    "        for p, choices in zip(pred_int, dev_choices)\n",
    "    ) / len(dev_choices)\n",
    "    return {\n",
    "        'spearman_int_vs_avg': float(spearman_corr) if spearman_corr == spearman_corr else 0.0,\n",
    "        'acc_within_sd': float(acc_within_sd),\n",
    "    }\n",
    "\n",
    "# --- Class-weighted loss (helps with label imbalance)\n",
    "counts = Counter(train_tok['labels'])\n",
    "freq = np.asarray([counts.get(i, 1) for i in range(5)], dtype=np.float32)\n",
    "w = (1.0 / (freq ** 0.75))\n",
    "w = w / w.mean()\n",
    "class_weights_t = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "_class_weights_cache = {}\n",
    "def compute_loss_func(outputs, labels, num_items_in_batch=None):\n",
    "    logits = outputs.get('logits')\n",
    "    device = logits.device\n",
    "    w_dev = _class_weights_cache.get(device)\n",
    "    if w_dev is None:\n",
    "        w_dev = class_weights_t.to(device)\n",
    "        _class_weights_cache[device] = w_dev\n",
    "    loss_fct = nn.CrossEntropyLoss(weight=w_dev)\n",
    "    return loss_fct(logits.view(-1, 5), labels.view(-1))\n",
    "\n",
    "# --- Model\n",
    "# IMPORTANT: do NOT load weights in float16 here. When fp16=True, Trainer uses AMP with a GradScaler,\n",
    "# and loading fp16 weights can trigger: \"Attempting to unscale FP16 gradients\".\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=5,\n",
    "    problem_type='single_label_classification',\n",
    ")\n",
    "\n",
    "# Batch settings (fast, but safe)\n",
    "PER_DEVICE_TRAIN_BS = 8 if use_cuda else 2\n",
    "GRAD_ACCUM = 1 if use_cuda else 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # output_dir=str(PROJECT_ROOT / 'transformer_runs_best'),\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
    "    per_device_eval_batch_size=8 if use_cuda else 4,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type='cosine',\n",
    "    fp16=use_cuda,\n",
    "    bf16=False,\n",
    "    tf32=use_cuda,\n",
    "    optim='adamw_torch',\n",
    "    dataloader_pin_memory=use_cuda,\n",
    "    dataloader_num_workers=0,\n",
    "    # Train based on acc_within_sd: evaluate/save each epoch and keep the best checkpoint\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='acc_within_sd',\n",
    "    greater_is_better=True,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=200,\n",
    "    report_to=[],\n",
    "    seed=SEED,\n",
    " )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=dev_tok,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_loss_func=compute_loss_func,\n",
    "    compute_metrics=compute_metrics,\n",
    " )\n",
    "\n",
    "print(f\"Training {MODEL_NAME} (select best by acc_within_sd) | bs={PER_DEVICE_TRAIN_BS} | grad_accum={GRAD_ACCUM} | epochs={NUM_EPOCHS} ...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate best checkpoint (Trainer will have loaded it)\n",
    "eval_result = trainer.evaluate()\n",
    "print('\\n=== Final Evaluation (best checkpoint) ===')\n",
    "acc_pct = eval_result['eval_acc_within_sd'] * 100\n",
    "print(f\"Accuracy within SD: {eval_result['eval_acc_within_sd']:.4f} ({acc_pct:.1f}%)\")\n",
    "print(f\"Spearman correlation: {eval_result['eval_spearman_int_vs_avg']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce5768",
   "metadata": {},
   "source": [
    "## 6) Generate predictions.jsonl\n",
    "Export dev predictions as required by the SemEval format: one JSON per line with `id` and integer `prediction` in [1..5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db78c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits stats (min/max): -3.254697799682617 3.5358307361602783\n",
      "Prediction distribution: {1: 22, 2: 112, 3: 112, 4: 194, 5: 148}\n",
      "Gold (rounded avg) distribution: {1: 68, 2: 133, 3: 145, 4: 147, 5: 95}\n",
      "0 gold_avg= 3.6 pred= 4\n",
      "1 gold_avg= 3.6 pred= 4\n",
      "2 gold_avg= 3.8 pred= 3\n",
      "3 gold_avg= 4.2 pred= 5\n",
      "4 gold_avg= 3.0 pred= 5\n",
      "5 gold_avg= 3.0 pred= 4\n",
      "6 gold_avg= 4.6 pred= 5\n",
      "7 gold_avg= 1.3333333333333333 pred= 1\n",
      "8 gold_avg= 2.2 pred= 2\n",
      "9 gold_avg= 3.8 pred= 4\n",
      "Wrote predictions: d:\\Fac\\Fac\\RN\\CARN_project\\predictions.jsonl\n",
      "Also wrote predictions for scoring: d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\input\\res\\predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "pred_out = trainer.predict(dev_tok)\n",
    "logits = pred_out.predictions  # (N,5)\n",
    "pred_class = np.argmax(logits, axis=-1)\n",
    "pred_int = (pred_class + 1).tolist()\n",
    "\n",
    "print('Logits stats (min/max):', float(np.min(logits)), float(np.max(logits)))\n",
    "print('Prediction distribution:', {i: pred_int.count(i) for i in range(1, 6)})\n",
    "print('Gold (rounded avg) distribution:', {i: sum(clip_round_to_1_5(a) == i for a in dev_avg) for i in range(1, 6)})\n",
    "\n",
    "# Quick sanity-check: show first few (id, gold avg, pred)\n",
    "for sample_id, gold_avg, pred in list(zip(dev_ids, dev_avg, pred_int))[:10]:\n",
    "    print(sample_id, 'gold_avg=', gold_avg, 'pred=', pred)\n",
    "\n",
    "def write_predictions_jsonl(ids: list[str], preds: list[int], out_path: Path) -> None:\n",
    "    assert len(ids) == len(preds)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open('w', encoding='utf-8', newline='\\n') as f:\n",
    "        for sample_id, pred in zip(ids, preds):\n",
    "            f.write(json.dumps({'id': str(sample_id), 'prediction': int(pred)}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Required by your prompt\n",
    "out_predictions_root = PROJECT_ROOT / 'predictions.jsonl'\n",
    "write_predictions_jsonl(dev_ids, pred_int, out_predictions_root)\n",
    "print('Wrote predictions:', out_predictions_root)\n",
    "\n",
    "# Optional: also write where the SemEval scripts expect it\n",
    "out_predictions_scorer = PROJECT_ROOT / 'semeval26-05-scripts' / 'input' / 'res' / 'predictions.jsonl'\n",
    "write_predictions_jsonl(dev_ids, pred_int, out_predictions_scorer)\n",
    "print('Also wrote predictions for scoring:', out_predictions_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf496c6",
   "metadata": {},
   "source": [
    "## 7) (Optional) Run official scorer\n",
    "This validates formatting and reports official metrics on dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f9c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: d:\\Fac\\Fac\\RN\\CARN_project\\.venv\\Scripts\\python.exe d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\scoring.py d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\input\\ref\\solution.jsonl d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\input\\res\\predictions.jsonl d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\output\\scores.json\n",
      "Importing...\n",
      "Starting Scoring script...\n",
      "Everything looks OK. Evaluating file d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\input\\res\\predictions.jsonl on d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\input\\ref\\solution.jsonl\n",
      "----------\n",
      "Spearman Correlation: 0.5092465344174596\n",
      "Spearman p-Value: 4.059729896385031e-40\n",
      "----------\n",
      "Accuracy: 0.6581632653061225 (387/588)\n",
      "Results dumped into scores.json successfully.\n",
      "\n",
      "\n",
      "Scores JSON: d:\\Fac\\Fac\\RN\\CARN_project\\semeval26-05-scripts\\output\\scores.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "scoring_script = PROJECT_ROOT / 'semeval26-05-scripts' / 'scoring.py'\n",
    "gold = PROJECT_ROOT / 'semeval26-05-scripts' / 'input' / 'ref' / 'solution.jsonl'\n",
    "preds = PROJECT_ROOT / 'semeval26-05-scripts' / 'input' / 'res' / 'predictions.jsonl'\n",
    "scores_out = PROJECT_ROOT / 'semeval26-05-scripts' / 'output' / 'scores.json'\n",
    "scores_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cmd = [\n",
    "    str(Path(sys.executable)),\n",
    "    str(scoring_script),\n",
    "    str(gold),\n",
    "    str(preds),\n",
    "    str(scores_out),\n",
    "]\n",
    "\n",
    "print('Running:', ' '.join(cmd))\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "print(result.stderr)\n",
    "print('Scores JSON:', scores_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
